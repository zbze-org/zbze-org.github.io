# Что нужно чтобы эффективно работать с текстом?

- Очистка.
- Спеллчекинг.
Есть наработки по классификатору RandomForest, обученному на синтеческих данных с имитацией опечаток и галлюцинаций OCR.
- Токенизация.
Есть наработки по использованию tokenizers от HuggingFace, Unigram хорошо отработает для разделения слов на морфемы, вполне соблюдая правила кабардино-черкесского языка.
- Стемминг.
Есть наработки с использованием токенизатора и префиксного дерева
- Лемматизация.
Есть наработки с токенизатором, префиксным деревом, поиска словарных форм по векторным сходствам и растояниям Левенштейна.
- Визуализация данных.
Есть наработки по визуализации данных с токенизатором и FSM. Цепи Маркова для анализа последовательностей слов.

Желательно иметь:
- Детекция языка.
Есть наработки с обучением классификатора FastText (KBD, ADY, RUS), но нужно хорошенько выровнять чтобы миинимизировать предвзятость.
- Исправление опечаток.
Есть наработки с использованием Левенштейна и индексом триграмм для поиска ближайших слов в "подготовленном" словаре.
- Разметка по частям речи (POS).
Были попытки сделать каскадный POS теггер из NLTK. На данных из словарей, regex и правил. Не особо пока вышло   
- Выделение именованных сущностей (NER).
- Анализ зависимостей (Dependency Parsing).
